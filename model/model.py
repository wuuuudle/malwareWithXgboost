import pandas as pd
from sklearn.model_selection import KFold
from sklearn.externals import joblib
from sklearn.model_selection import train_test_split
import xgboost as xgb
import numpy as np

'''将训练数据进行K折划分，并保存，正式训练时直接读取
传入参数:
    folds：即K，指定划分的折数
'''


def dataPartition(folds=5):
    features = pd.read_csv('data/integrated_features_dynamic.csv')
    # 测试原来正负样本分布
    num = len(features)
    norm_num, abnorm_num = len(features[features.label == 0]), len(features[features.label == 1])
    print('原样本总数 : %d\n正样本数 : %d-%.3f，负样本 : %d-%.3f\n' % (num, norm_num, norm_num / num, abnorm_num, abnorm_num / num))

    # 使用sklearn进行K折划分
    Kfold = KFold(n_splits=folds, shuffle=True, random_state=0)
    cnt = 0
    for train_idx, test_idx in Kfold.split(features):
        train, test = features.iloc[train_idx, :], features.iloc[test_idx, :]
        cnt += 1

        print('第%d折分布' % cnt)
        # 测试划分后正负样本分布
        num = len(test)
        norm_num, abnorm_num = len(test[test.label == 0]), len(test[test.label == 1])
        print('划分后样本总数：%d\n正样本数：%d-%.3f, 负样本：%d-%.3f\n' % (num, norm_num, norm_num / num, abnorm_num, abnorm_num / num))
        test.to_csv('data/test_dataset_%d.csv' % cnt, index=False)
        train.to_csv('data/train_dataset_%d.csv' % cnt, index=False)


def getSeasonDataset(times=None, test_without_0=True, fold=1):
    train_data = pd.read_csv(r'data/train_dataset_%d.csv' % fold, encoding='utf8')
    test_data = pd.read_csv(r'data/test_dataset_%d.csv' % fold)

    if times != None:
        norm = train_data[train_data['label'] == 0]
        abnorm = train_data[train_data['label'] == 1]
        norm = norm.sample(n=int(len(abnorm) * times), random_state=0)
        train_data = norm.append(abnorm).sample(frac=1)

    return train_data.drop('label', axis=1), test_data.drop('label', axis=1), \
           pd.DataFrame(train_data['label'], columns=['label']), pd.DataFrame(test_data['label'], columns=['label'])


def printRes(y_true, y_pred):
    # 处理数据类型问题
    if isinstance(y_true, pd.DataFrame) or isinstance(y_true, pd.Series):
        y_true = y_true.values.reshape(-1)
    fn, fp = 0, 0
    tn, tp = 0, 0

    # 分别对四类样本进行计数
    for i in range(len(y_true)):
        if (y_true[i] != y_pred[i]):
            # print("y_test:%d, y_pred:%d\n"%(test[i],test_y_pred[i]))
            if y_pred[i] == 0:
                fn += 1
            else:
                fp += 1
        else:
            if y_pred[i] == 0:
                tn += 1
            else:
                tp += 1
    # 打印计数结果
    print("tn:%7d, fn:%5d" % (tn, fn))
    print("fp:%7d, tp:%5d" % (fp, tp))
    print("f1-score:%.3f" % (2 * tp / (2 * tp + fn + fp)))
    return 2 * tp / (2 * tp + fn + fp)


def ensemble(probas, weights):
    y = 0
    # 加权求和
    for i in range(len(weights)):
        y = y + probas[i] * weights[i] * weights[i] * weights[i]
    # 权重归一化
    y = y / np.sum(np.power(weights, 3))
    y = (y > 0.45).astype(np.int32)
    return y


def train_model(train_mode='all', save_model=False, folds=5):
    all_weights = []
    for fold in range(1, folds + 1):
        # 训练第一层分类器
        if train_mode == 'c1' or train_mode == 'all':
            # 获取特征并训练
            x_train, x_val, y_train, y_val = getSeasonDataset(fold=fold)
            print('\n\n====================第%d折=========================' % fold)
            print('特征数：%d' % (x_train.shape[1]))
            clf = xgb.XGBClassifier(scale_pos_weight=100, learning_rate=0.03, n_estimators=100, max_depth=6,
                                    min_child_weight=10,
                                    gamma=0.3, subsample=0.3, reg_alpha=10, seed=0, n_jobs=6)
            clf.fit(x_train, y_train)
            train_y_pred_1 = clf.predict(x_train)
            val_y_pred_1 = clf.predict(x_val)
            if save_model:
                joblib.dump(clf, 'model/%d_xgb_c1.m' % fold, compress=3)
            print('C1训练集效果：')
            printRes(y_train, train_y_pred_1)
            print('C1验证集效果：')
            printRes(y_val, val_y_pred_1)

        # 训练第二层分类器
        if train_mode == 'C2' or train_mode == 'all':
            # 获取C2的训练数据
            x_train['label'] = y_train
            x_train['pred'] = train_y_pred_1
            x_temp = x_train[x_train.pred == 1]
            x_train.drop('pred', axis=1)
            x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(x_temp.drop( \
                ['label', 'pred'], axis=1), x_temp['label'], test_size=0.1, random_state=0)
            # 定义并训练C2分类器
            clf2 = xgb.XGBClassifier(scale_pos_weight=18, learning_rate=0.1, n_estimators=110, max_depth=7,
                                     min_child_weight=8,
                                     gamma=0.5, subsample=0.25, colsample_bytree=0.6, reg_alpha=8, seed=0, n_jobs=6)

            clf2.fit(x_train_2, y_train_2)
            train_y_pred_2 = clf2.predict(x_train_2)
            test_y_pred_2 = clf2.predict(x_test_2)
            val_y_pred_2 = clf2.predict(x_val)

            # 保存模型
            if save_model:
                joblib.dump(clf2, 'model/%d_xgb_c2.m' % fold, compress=3)

            print('\nC2训练集效果：')
            printRes(y_train_2, train_y_pred_2)
            print('C2测试集效果：')
            printRes(y_test_2, test_y_pred_2)
            print('C2验证集效果：')
            printRes(y_val, val_y_pred_2)

            # 打印C1C2级联后的效果
            print('C1、C2级联后:')
            c1_c2 = np.multiply(val_y_pred_1, val_y_pred_2)
            printRes(y_val, c1_c2)

        # 训练第三层分类器
        if train_mode == 'C3' or train_mode == 'all':
            # 获得C3的训练数据
            x_train_2['label'] = y_train_2
            x_train_2['pred'] = train_y_pred_2
            x_test_2['label'] = y_test_2
            x_test_2['pred'] = test_y_pred_2
            x_train_2.append(x_test_2)
            c1_c2_res = x_train_2.append(x_test_2)
            x_temp_2 = c1_c2_res[c1_c2_res.pred == 1]
            x_train_3, x_test_3, y_train_3, y_test_3 = train_test_split(x_temp_2.drop( \
                ['label', 'pred'], axis=1), x_temp_2['label'], test_size=0.2, random_state=0)

            # 定义并训练C3分类器
            ## C3_1
            clf3_1 = xgb.XGBClassifier(scale_pos_weight=3.3, learning_rate=0.03, n_estimators=300, max_depth=16,
                                       min_child_weight=4,
                                       gamma=0.3, subsample=0.5, colsample_bytree=0.6, reg_alpha=5, seed=0,
                                       n_jobs=6)
            clf3_1.fit(x_train_3, y_train_3)
            proba = clf3_1.predict_proba(x_val)[:, 1]
            c1_c2_c3_1 = np.multiply(c1_c2, proba)  # 级联后的概率
            pred = clf3_1.predict(x_val)  # 计算验证集F1-Score
            pred_1 = np.multiply(c1_c2, pred)
            print('C3 xgboost1效果')
            all_weights.append(printRes(y_val, pred_1))
            ## C3_2
            clf3_2 = xgb.XGBClassifier(scale_pos_weight=3, learning_rate=0.01, n_estimators=500, max_depth=10,
                                       min_child_weight=5,
                                       gamma=0.3, subsample=0.5, colsample_bytree=0.6, reg_alpha=2, seed=0,
                                       n_jobs=6)
            clf3_2.fit(x_train_3, y_train_3)
            proba = clf3_2.predict_proba(x_val)[:, 1]
            c1_c2_c3_2 = np.multiply(c1_c2, proba)  # 级联后的概率
            pred = clf3_2.predict(x_val)  # 计算验证集F1-Score
            pred_2 = np.multiply(c1_c2, pred)
            print('C3 xgboost2效果')
            all_weights.append(printRes(y_val, pred_2))
            ## C3_3
            clf3_3 = xgb.XGBClassifier(scale_pos_weight=2.8, learning_rate=0.1, n_estimators=200, max_depth=10,
                                       min_child_weight=5,
                                       gamma=0.3, subsample=0.5, colsample_bytree=0.6, reg_alpha=10, seed=0,
                                       n_jobs=6)
            clf3_3.fit(x_train_3, y_train_3)
            proba = clf3_3.predict_proba(x_val)[:, 1]
            c1_c2_c3_3 = np.multiply(c1_c2, proba)  # 级联后的概率
            pred = clf3_3.predict(x_val)  # 计算验证集F1-Score
            pred_3 = np.multiply(c1_c2, pred)
            print('C3 xgboost3效果')
            all_weights.append(printRes(y_val, pred_3))

            # 保存模型
            if save_model:
                joblib.dump(clf3_1, 'model/%d_xgb_c3_1.m' % fold, compress=3)
                joblib.dump(clf3_2, 'model/%d_xgb_c3_2.m' % fold, compress=3)
                joblib.dump(clf3_3, 'model/%d_xgb_c3_3.m' % fold, compress=3)

            probas, weights = [c1_c2_c3_1, c1_c2_c3_2, c1_c2_c3_1], all_weights[-3:]
            final_pred = ensemble(probas, weights)

            print('最终效果：')
            printRes(y_val, final_pred)

    # 保存模型权重
    with open('model/weights.txt','w') as file:
        file.write(str(all_weights))
